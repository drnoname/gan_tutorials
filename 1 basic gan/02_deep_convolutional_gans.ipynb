{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "## 1. Activation Functions\n",
    "\n",
    "确保：non-linear and differentiable\n",
    "\n",
    "### 1.2 一些常用的Activation Functions\n",
    "\n",
    "#### 1. ReLU\n",
    "\n",
    "#### 2. Leaky ReLU\n",
    "\n",
    "Dying ReLU problem: 当activation 输出是0时，一些node 会get stuck and stop learning. 这些node 之前的一些node 也会收到影响。\n",
    "\n",
    "但这是否相当于dropout?\n",
    "\n",
    "slop: parameter a\n",
    "\n",
    "#### 3. Sigmoid\n",
    "\n",
    "因为输出在0-1之间，所以通常用于binary classification的最后一层。\n",
    "\n",
    "但是通常会带来vanishing gradient and saturation problems.\n",
    "\n",
    "#### 4. Tanh\n",
    "\n",
    "类似sigmoid，输出在-1 到1之间。和sigmoid 的区别在于：tanh function keeps the sign of input. 输入是正的，输出也是正的。\n",
    "\n",
    "因为shape 和sigmoid 很像，所以也会有vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Normalization (这部分讲的不好) - refer to c2w3 Andrew Ng\n",
    "\n",
    "通常，训练一个GAN 需要很长时间。同时GAN 的训练很fragile.所以我们需要一些加速训练和stabilize 训练的方法，本章我们介绍Batch Normalization.\n",
    "\n",
    "### 2.1 为什么需要Normalization\n",
    "\n",
    "如果输入的分布不同，那么cost function will be elongated。例如，我们有一个神经元的neural network，接收一个人的身高($x_1$)和体重($x_2$)，来区分这是个男人还是女人。\n",
    "\n",
    "假设$x_1$和$x_2$的分布不同，例如下图所示。\n",
    "\n",
    "<img src=\"figures/input_distribution.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "我们最后得到的cost function 则会是一个变形的，如下图所示。\n",
    "\n",
    "<img src=\"figures/elongated_cost_fnction.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "#### Covariate Shift\n",
    "\n",
    "Changes in the distribution of one variable affect the distribution of related variables. \n",
    "当一个数据的分布发生变化时，就会带来covariate shift.\n",
    "例如，我们可以通过对输入数据进行normalization 来改变cost function 的形状，这就是一个covariate shift.\n",
    "\n",
    "<img src=\"figures/normalized.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "通过normalization the cost function will also look smoother and more balanced across these two dimensions. And as a result training would actually be much easier and potentially much faster.\n",
    "\n",
    "### 2.2 Batch Normalization\n",
    "对于一个神经网络，会受到internal covariate shift 的影响。所以我们引入Batch Normalization which normalizes the input for each neuron.\n",
    "\n",
    "### 2.3 how to do it\n",
    "\n",
    "#### Batch normalization for training\n",
    "\n",
    "做这个normalization 的时候，对于训练数据，我们使用batch statistics，对于testing data，我们使用训练数据的statistics.\n",
    "\n",
    "And using normalization, the effect of this covariate shift will be reduced significantly.\n",
    "\n",
    "#### Batch normalization for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review of Convolutions\n",
    "\n",
    "CNN 在图像处理中还是主流，所以很多GAN architecture 中都有CNN 的影子，这一节我们来回顾以下CNN.\n",
    "CNN 主要的作用是做图像的特征检测。\n",
    "\n",
    "- filters: \n",
    "    - 不同的filter 可以检测不同的特征\n",
    "    - 可以同时使用多个filter\n",
    "    - 每个filter 是一个matrix\n",
    "- Stride: 每次移动多少个pixel\n",
    "    \n",
    "- Padding: \n",
    "    - 解决问题： convolution 的一个问题是，边界的pixel 被遍历到的次数要少于中间的pixel，所以边上的特征可能会提取不到。you want to make sure your filter puts equal emphasis across the entire image\n",
    "    - zero padding: 补0\n",
    "    \n",
    "- Pooling: reduce the size of input\n",
    "    - mean, max, min, etc.\n",
    "    - 通常使用max pooling，原因是capture the most salient information\n",
    "    - Pooling 没有learnable parameters\n",
    "\n",
    "- Upsampling: increase the size of input\n",
    "    - linear interpolation\n",
    "    - bi-linear interpolation\n",
    "    - 和pooling 一样，upsampling 也没有learnable parameters\n",
    "\n",
    "#### Transposed Convolutions\n",
    "假设我们又一个2*2 的input，有一个2*2 的filter，如果做没有padding 的convolution 我们会得到一个scalar。如果我们做transposed convolution，我们会得到一个3*3 的矩阵，如下图所示。\n",
    "\n",
    "<img src=\"figures/transposed_convolution.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "这样做的一个问题是，upsampling 之后，中间的点收到更多的影响，会产生**checkerboard pattern**. 通常可以使用一个upsampling + convolution layer 来解决这checkerboard problem. \n",
    "\n",
    "有关更多的checkerboard problem 可参见[1].\n",
    "\n",
    "transposed convolution (通常也叫deconvolution) learns a filter to upsample.\n",
    "- transposed convolution 是一个upsampling 的方法\n",
    "- 有learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Checkerboard pattern\n",
    "\n",
    "以下内容来自[1]. \n",
    "\n",
    "许多生成的图片都有checkerboard pattern，尤其是在深色（strong colors）区域。下面我们分析以下原因。\n",
    "\n",
    "<img src=\"figures/checkerboard_pattern.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n",
    "\n",
    "[2] [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n",
    "\n",
    "[3] [Video Generation with TGAN] https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W2_Video_Generation_(Optional).ipynb\n",
    "\n",
    "[4] Temporal Generative Adversarial Nets with Singular Value Clipping\n",
    "\n",
    "[5] MNIST Database: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
